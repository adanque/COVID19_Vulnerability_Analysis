#Author: Alan Danque
#COVI19 Vulnerability analysis 
#Graphics Analysis

from warnings import simplefilter
# ignore all future warnings
simplefilter(action='ignore', category=FutureWarning)

import pandas as pd
import yellowbrick

print("Starting Step 1 Here ")
#Step 1:  Load data into a dataframe
addr1 = "Train_Covid19_Data_Week_11.csv" 
data = pd.read_csv(addr1)

print("Starting Step 2 Here ")
# Step 2:  check the dimension of the table
print("The dimension of the table is: ", data.shape)

print("Starting Step 3 Here ")
#Step 3:  Look at the data
print(data.head(5))

print("Starting Step 5 Here ")
#Step 5:  what type of variables are in the table
print("Describe Data")
print(data.describe())
print("Summarized Data")
print(data.describe(include=['O']))


print("Starting Step 6 Here ")
#Step 6: import visulization packages
import matplotlib.pyplot as plt

# set up the figure size
plt.rcParams['figure.figsize'] = (20, 10)

# make subplots
fig, axes = plt.subplots(nrows = 2, ncols = 2)

# Specify the features of interest
# num_features = ['Age', 'SibSp', 'Parch', 'Fare']
num_features = ['NumberOfDays', 'Confirmed_Cases_Count', 'Death_Count', 'Recovered_Cases_Count']

xaxes = num_features
# yaxes = ['Counts', 'Counts', 'Counts', 'Counts']
yaxes = ['Counts', 'Counts', 'Counts', 'Counts']

# draw histograms
axes = axes.ravel()
for idx, ax in enumerate(axes):
    ax.hist(data[num_features[idx]].dropna(), bins=40)
    ax.set_xlabel(xaxes[idx], fontsize=20)
    ax.set_ylabel(yaxes[idx], fontsize=20)
    ax.tick_params(axis='both', labelsize=15)
plt.show()

print("Starting Step 7 Here ")
#7:  Barcharts: set up the figure size
#%matplotlib inline
plt.rcParams['figure.figsize'] = (20, 10)

# make subplots
fig, axes = plt.subplots(nrows = 2, ncols = 2)

# make the data read to feed into the visulizer
#X_Survived = data.replace({'Survived': {1: 'yes', 0: 'no'}}).groupby('Survived').size().reset_index(name='Counts')['Survived']
#Y_Survived = data.replace({'Survived': {1: 'yes', 0: 'no'}}).groupby('Survived').size().reset_index(name='Counts')['Counts']
X_Survived = data.replace({'BendCurve': {1: 'yes', 0: 'no'}}).groupby('BendCurve').size().reset_index(name='Counts')['BendCurve']
Y_Survived = data.replace({'BendCurve': {1: 'yes', 0: 'no'}}).groupby('BendCurve').size().reset_index(name='Counts')['BendCurve']

# make the bar plot
axes[0, 0].bar(X_Survived, Y_Survived)
axes[0, 0].set_title('BendCurve', fontsize=25)
axes[0, 0].set_ylabel('Counts', fontsize=20)
axes[0, 0].tick_params(axis='both', labelsize=15)

# make the data read to feed into the visulizer
X_Pclass = data.replace({'Over_30_60_Days': {1: '1st', 2: '2nd', 3: '3rd'}}).groupby('Over_30_60_Days').size().reset_index(name='Counts')['Over_30_60_Days']
Y_Pclass = data.replace({'Over_30_60_Days': {1: '1st', 2: '2nd', 3: '3rd'}}).groupby('Over_30_60_Days').size().reset_index(name='Counts')['Counts']
# make the bar plot
axes[0, 1].bar(X_Pclass, Y_Pclass)
axes[0, 1].set_title('Over_30_60_Days', fontsize=25)
axes[0, 1].set_ylabel('Counts', fontsize=20)
axes[0, 1].tick_params(axis='both', labelsize=15)

# make the data read to feed into the visulizer
X_Sex = data.groupby('Governor').size().reset_index(name='Counts')['Governor']
Y_Sex = data.groupby('Governor').size().reset_index(name='Counts')['Counts']
# make the bar plot
axes[1, 0].bar(X_Sex, Y_Sex)
axes[1, 0].set_title('Governor', fontsize=25)
axes[1, 0].set_ylabel('Counts', fontsize=20)
axes[1, 0].tick_params(axis='both', labelsize=15)

# make the data read to feed into the visulizer
X_Embarked = data.groupby('Senior US Senator').size().reset_index(name='Counts')['Senior US Senator']
Y_Embarked = data.groupby('Senior US Senator').size().reset_index(name='Counts')['Counts']
# make the bar plot
axes[1, 1].bar(X_Embarked, Y_Embarked)
axes[1, 1].set_title('Senior US Senator', fontsize=25)
axes[1, 1].set_ylabel('Counts', fontsize=20)
axes[1, 1].tick_params(axis='both', labelsize=15)
plt.show()

print("Starting Step 8 Here ")
#Step 8: Pearson Ranking
#set up the figure size
#%matplotlib inline
plt.rcParams['figure.figsize'] = (15, 7)

# import the package for visulization of the correlation
from yellowbrick.features import Rank2D

# extract the numpy arrays from the data frame
#X = data[num_features].as_matrix()
X = data[num_features].to_numpy()

# instantiate the visualizer with the Covariance ranking algorithm
visualizer = Rank2D(features=num_features, algorithm='pearson')
visualizer.fit(X)                # Fit the data to the visualizer
visualizer.transform(X)             # Transform the data
visualizer.poof(outpath="pcoords1.png") # Draw/show/poof the data
plt.show()

print("Starting Step 9 Here ")
# Step 9:  Compare variables against Survived and Not Survived
#set up the figure size
#%matplotlib inline
plt.rcParams['figure.figsize'] = (15, 7)
plt.rcParams['font.size'] = 50

# setup the color for yellowbrick visulizer
from yellowbrick.style import set_palette
set_palette('sns_bright')

# import packages
from yellowbrick.features import ParallelCoordinates
# Specify the features of interest and the classes of the target
classes = ['Not-BendCurve', 'BendCurve']
num_features = ['NumberOfDays', 'Confirmed_Cases_Count', 'Death_Count', 'Recovered_Cases_Count']

# copy data to a new dataframe
data_norm = data.copy()
# normalize data to 0-1 range
for feature in num_features:
    data_norm[feature] = (data[feature] - data[feature].mean(skipna=True)) / (data[feature].max(skipna=True) - data[feature].min(skipna=True))

# Extract the numpy arrays from the data frame
# X = data_norm[num_features].as_matrix()
# y = data.Survived.as_matrix()
X = data_norm[num_features].to_numpy()
y = data.BendCurve.to_numpy()

# Instantiate the visualizer
visualizer = ParallelCoordinates(classes=classes, features=num_features)


visualizer.fit(X, y)      # Fit the data to the visualizer
visualizer.transform(X)   # Transform the data
visualizer.poof(outpath="pcoords2.png") # Draw/show/poof the data
plt.show();

print("Starting Step 10 Here ")
# Step 10 - stacked bar charts to compare survived/not survived
#set up the figure size
#%matplotlib inline
plt.rcParams['figure.figsize'] = (20, 10)

# make subplots
fig, axes = plt.subplots(nrows = 2, ncols = 2)

# make the data read to feed into the visulizer
Sex_survived = data.replace({'BendCurve': {1: 'BendCurve', 0: 'Not-BendCurve'}})[data['BendCurve']==1]['Governor'].value_counts()
Sex_not_survived = data.replace({'BendCurve': {1: 'BendCurve', 0: 'Not-BendCurve'}})[data['BendCurve']==0]['Governor'].value_counts()
Sex_not_survived = Sex_not_survived.reindex(index = Sex_survived.index)
# make the bar plot
p1 = axes[0, 0].bar(Sex_survived.index, Sex_survived.values)
p2 = axes[0, 0].bar(Sex_not_survived.index, Sex_not_survived.values, bottom=Sex_survived.values)
axes[0, 0].set_title('Governor', fontsize=25)
axes[0, 0].set_ylabel('Counts', fontsize=20)
axes[0, 0].tick_params(axis='both', labelsize=15)
axes[0, 0].legend((p1[0], p2[0]), ('BendCurve', 'Not-BendCurve'), fontsize = 15)

# make the data read to feed into the visualizer
Pclass_survived = data.replace({'BendCurve': {1: 'BendCurve', 0: 'Not-BendCurve'}}).replace({'Over_30_60_Days': {1: '1st', 2: '2nd', 3: '3rd'}})[data['BendCurve']==1]['Over_30_60_Days'].value_counts()
Pclass_not_survived = data.replace({'BendCurve': {1: 'BendCurve', 0: 'Not-BendCurve'}}).replace({'Over_30_60_Days': {1: '1st', 2: '2nd', 3: '3rd'}})[data['BendCurve']==0]['Over_30_60_Days'].value_counts()
Pclass_not_survived = Pclass_not_survived.reindex(index = Pclass_survived.index)
# make the bar plot
p3 = axes[0, 1].bar(Pclass_survived.index, Pclass_survived.values)
p4 = axes[0, 1].bar(Pclass_not_survived.index, Pclass_not_survived.values, bottom=Pclass_survived.values)
axes[0, 1].set_title('Over_30_60_Days', fontsize=25)
axes[0, 1].set_ylabel('Counts', fontsize=20)
axes[0, 1].tick_params(axis='both', labelsize=15)
axes[0, 1].legend((p3[0], p4[0]), ('BendCurve', 'Not-BendCurve'), fontsize = 15)

# make the data read to feed into the visualizer
Embarked_survived = data.replace({'BendCurve': {1: 'BendCurve', 0: 'Not-BendCurve'}})[data['BendCurve']==1]['Senior US Senator'].value_counts()
Embarked_not_survived = data.replace({'BendCurve': {1: 'BendCurve', 0: 'Not-BendCurve'}})[data['BendCurve']==0]['Senior US Senator'].value_counts()
Embarked_not_survived = Embarked_not_survived.reindex(index = Embarked_survived.index)
# make the bar plot
p5 = axes[1, 0].bar(Embarked_survived.index, Embarked_survived.values)
p6 = axes[1, 0].bar(Embarked_not_survived.index, Embarked_not_survived.values, bottom=Embarked_survived.values)
axes[1, 0].set_title('Senior US Senator', fontsize=25)
axes[1, 0].set_ylabel('Counts', fontsize=20)
axes[1, 0].tick_params(axis='both', labelsize=15)
axes[1, 0].legend((p5[0], p6[0]), ('BendCurve', 'Not-BendCurve'), fontsize = 15)
plt.show()

print(" The End  of Part 1 ")

print("Starting step 11")
# Step 11 - fill in missing values and eliminate features
#fill the missing age data with median value
def fill_na_median(data, inplace=True):
    return data.fillna(data.median(), inplace=inplace)

fill_na_median(data['NumberOfDays'])

# check the result
print("NumberOfDays column of the data dataframe.")
print(data['NumberOfDays'].describe())

# fill with the most represented value
def fill_na_most(data, inplace=True):
    return data.fillna('Democratic', inplace=inplace)

fill_na_most(data['Senior US Senator'])

# check the result
print("Senior US Senator column of the data dataframe.")
print(data['Senior US Senator'].describe())

# import package
import numpy as np

# log-transformation
def log_transformation(data):
    return data.apply(np.log1p)

data['Recovered_Cases_Count_log1p'] = log_transformation(data['Recovered_Cases_Count'])

# check the data
print("Data dataframe.")
print(data.describe())

print("Starting step 12")

#Step 12 - adjust skewed data (Recovered_Cases_Count)
#check the distribution using histogram
# set up the figure size
#%matplotlib inline
plt.rcParams['figure.figsize'] = (10, 5)

plt.hist(data['Recovered_Cases_Count_log1p'], bins=40)
plt.xlabel('Recovered_Cases_Count_log1p', fontsize=20)
plt.ylabel('Counts', fontsize=20)
plt.tick_params(axis='both', labelsize=15)
plt.show()


print("Starting step 13")
#Step 13 - convert categorical data to numbers
#get the categorical data
cat_features = ['Over_30_60_Days', 'Governor', "Senior US Senator"]
data_cat = data[cat_features]
data_cat = data_cat.replace({'Over_30_60_Days': {1: '1st', 2: '2nd', 3: '3rd'}})
# One Hot Encoding
print("One Hot Encoding Matrix")
data_cat_dummies = pd.get_dummies(data_cat)
# check the data
print(data_cat_dummies.head(8))
print("End of Step 13")

print(" The End  of Part 2 ")


print("Starting step 14")
#Step 14 - create a whole features dataset that can be used for train and validation data splitting
# here we will combine the numerical features and the dummie features together
features_model = ['NumberOfDays', 'Confirmed_Cases_Count', 'Death_Count', 'Recovered_Cases_Count_log1p']
#['Age', 'SibSp', 'Parch', 'Fare_log1p']
data_model_X = pd.concat([data[features_model], data_cat_dummies], axis=1)

# create a whole target dataset that can be used for train and validation data splitting
data_model_y = data.replace({'BendCurve': {1: 'BendCurve', 0: 'Not_BendCurve'}})['BendCurve']
# separate data into training and validation and check the details of the datasets
# import packages
from sklearn.model_selection import train_test_split

# split the data
X_train, X_val, y_train, y_val = train_test_split(data_model_X, data_model_y, test_size =0.3, random_state=11)

# number of samples in each set
print("No. of samples in training set: ", X_train.shape[0])
print("No. of samples in validation set:", X_val.shape[0])

# Survived and not-survived
print('\n')
print('No. of BendCurve and not-BendCurve in the training set:')
print(y_train.value_counts())

print('\n')
print('No. of BendCurve and not-BendCurve in the validation set:')
print(y_val.value_counts())


print("Starting step 15")
# Step 15 - Eval Metrics
from sklearn.linear_model import LogisticRegression

from yellowbrick.classifier import ConfusionMatrix
from yellowbrick.classifier import ClassificationReport
from yellowbrick.classifier import ROCAUC

# Instantiate the classification model
model = LogisticRegression(solver='liblinear', max_iter=60)

#The ConfusionMatrix visualizer taxes a model
classes = ['Not_BendCurve','BendCurve']
cm = ConfusionMatrix(model, fontsize=13, classes=classes, percent=False)

#Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model
cm.fit(X_train, y_train)

#To create the ConfusionMatrix, we need some test data. Score runs predict() on the data
#and then creates the confusion_matrix from scikit learn.
cm.score(X_val, y_val)

# change fontsize of the labels in the figure
for label in cm.ax.texts:
    label.set_size(20)

#How did we do?
cm.poof(bbox_inches='tight')

# Precision, Recall, and F1 Score
# set the size of the figure and the font size
plt.tight_layout(rect=[.5, 0.5, .5, 0.05])
plt.rcParams['figure.figsize'] = (15, 7)
plt.rcParams['font.size'] = 20



# Instantiate the visualizer
visualizer = ClassificationReport(model, classes=classes)

visualizer.fit(X_train, y_train)  # Fit the training data to the visualizer
visualizer.score(X_val, y_val)  # Evaluate the model on the test data
g = visualizer.poof()

# ROC and AUC
#Instantiate the visualizer
visualizer = ROCAUC(model)

visualizer.fit(X_train, y_train)  # Fit the training data to the visualizer
visualizer.score(X_val, y_val)  # Evaluate the model on the test data
g = visualizer.poof()

print("End of Part 3")

print("k-fold CV - Cross Validation Testing")
from sklearn import svm
X_train, X_val, y_train, y_val = train_test_split(data_model_X, data_model_y, test_size =0.3, random_state=11)
clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)
crossvalidate = clf.score(X_val, y_val)
print(crossvalidate)

print("cross_val_score")
from sklearn.model_selection import cross_val_score
clf = svm.SVC(kernel='linear', C=1)
scores = cross_val_score(clf, X_val, y_val, cv=2)
print(scores)
print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))

print("f1_macro")
from sklearn import metrics
scores = cross_val_score(clf, X_val, y_val, cv=2, scoring='f1_macro')
print(scores)


print("Data Transformation with Held out data")
from sklearn import preprocessing
X_train, X_val, y_train, y_val = train_test_split(data_model_X, data_model_y, test_size =0.4, random_state=0)
scaler = preprocessing.StandardScaler().fit(X_train)
X_train_transformed = scaler.transform(X_train)
clf = svm.SVC(C=1).fit(X_train_transformed, y_train)
X_test_transformed = scaler.transform(X_val)
heldout = clf.score(X_test_transformed, y_val)
print(heldout)


print("Pipeline estimator")
from sklearn.pipeline import make_pipeline
clf = make_pipeline(preprocessing.StandardScaler(), svm.SVC(C=1))
pipeline = cross_val_score(clf, X_val, y_val, cv=2)
print(pipeline)

print("KFold")
from sklearn.model_selection import KFold
kf = KFold(n_splits=2)
for train, test in kf.split(X_val):
    print("%s %s" % (train, test))


